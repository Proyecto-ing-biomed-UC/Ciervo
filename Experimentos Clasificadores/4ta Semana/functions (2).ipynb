{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Clasificador con Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:38.160283Z",
     "start_time": "2024-08-28T17:04:22.317983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ciervo in /opt/conda/lib/python3.8/site-packages (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "fatal: destination path 'balu3' already exists and is not an empty directory.\n",
      "Processing ./balu3\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Building wheels for collected packages: balu3\n",
      "  Building wheel for balu3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for balu3: filename=balu3-1.0-py3-none-any.whl size=43718 sha256=c455163420a44c325d1d72b267e1859d864b06bad1d2bdd368d6eb07cd223518\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y2rz1p5t/wheels/2e/72/90/0b6128ea07a8a26c59ffe83a05ebdf4c86dcf5b9aeefc3d561\n",
      "Successfully built balu3\n",
      "Installing collected packages: balu3\n",
      "  Attempting uninstall: balu3\n",
      "    Found existing installation: balu3 1.0\n",
      "    Uninstalling balu3-1.0:\n",
      "      Successfully uninstalled balu3-1.0\n",
      "Successfully installed balu3-1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ciervo --upgrade\n",
    "!git clone https://github.com/domingomery/balu3\n",
    "!pip install ./balu3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.064125Z",
     "start_time": "2024-08-28T17:04:38.166608Z"
    }
   },
   "outputs": [],
   "source": [
    "#balu3\n",
    "from balu3.fs.sel  import sfs, clean        \n",
    "from balu3.ft.norm import minmax \n",
    "\n",
    "#ciervo\n",
    "#from ciervo.io import example_marcha, example_marcha_larga\n",
    "\n",
    "#signals\n",
    "from scipy import signal\n",
    "import scipy.signal\n",
    "from scipy.signal import find_peaks\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np   \n",
    "\n",
    "#selector/transformer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from balu3.ft.trans import pca                                                 \n",
    "from sklearn.decomposition import FastICA  \n",
    "\n",
    "# classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestCentroid                           \n",
    "from sklearn.naive_bayes import GaussianNB                              \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "from sklearn.tree import DecisionTreeClassifier                         \n",
    "from sklearn.ensemble import RandomForestClassifier                     \n",
    "from sklearn.linear_model import LogisticRegression      \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    \n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier \n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics   import confusion_matrix, accuracy_score    \n",
    "\n",
    "#Redes Neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score    \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Selection & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_normalized_feature_selection(train_data, test_data):\n",
    "    sclean = clean(train_data)  # Indices of selected features\n",
    "    train_data, test_data = train_data[:, sclean], test_data[:, sclean]\n",
    "    train_data, a, b = minmax(train_data)\n",
    "    test_data = test_data * a + b\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seleccion 5,10,15,20,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.119851Z",
     "start_time": "2024-08-28T17:04:40.116216Z"
    }
   },
   "outputs": [],
   "source": [
    "def sfs_selection(train_data, test_data,train_labels, n_indices):\n",
    "    train_data, test_data = clean_normalized_feature_selection(train_data, test_data)\n",
    "    sfs_indices = sfs(train_data, train_labels, n_indices) \n",
    "    train_data = train_data[:,sfs_indices] \n",
    "    test_data  =  test_data[:,sfs_indices] \n",
    "    print(sfs_indices)\n",
    "    return train_data, test_data #,sfs_indices10,20,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,20,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.126977Z",
     "start_time": "2024-08-28T17:04:40.123238Z"
    }
   },
   "outputs": [],
   "source": [
    "def pca_tranformer(train_data, test_data,n_components):\n",
    "  train_data, test_data = clean_normalized_feature_selection(train_data, test_data)\n",
    "  train_data, _, A, Xm, _ = pca(train_data, n_components=n_components)\n",
    "  test_data = np.matmul(test_data- Xm, A)\n",
    "\n",
    "  train_data, a, b = minmax(train_data)\n",
    "  test_data        = test_data * a + b\n",
    "  return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,20,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.132812Z",
     "start_time": "2024-08-28T17:04:40.129733Z"
    }
   },
   "outputs": [],
   "source": [
    "def ica_tranformer(train_data, test_data, train_labels, n_components):\n",
    "  train_data, test_data = clean_normalized_feature_selection(train_data, test_data)\n",
    "  ica=FastICA(n_components=n_components, random_state=0)\n",
    "  ica.fit(train_data, train_labels)\n",
    "  train_data = ica.transform(train_data)\n",
    "  test_data = ica.transform(test_data)\n",
    "\n",
    "  return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,20,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.142289Z",
     "start_time": "2024-08-28T17:04:40.136317Z"
    }
   },
   "outputs": [],
   "source": [
    "def plsr_transformer(train_data, test_data, train_labels,n_components):\n",
    "  train_data, test_data = clean_normalized_feature_selection(train_data, test_data)\n",
    "  plsr = PLSRegression(n_components=n_components)\n",
    "  plsr.fit(train_data, train_labels)\n",
    "  train_data = plsr.transform(train_data)\n",
    "  test_data = plsr.transform(test_data)\n",
    "\n",
    "  return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Clasification & Evaluation, Con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.157337Z",
     "start_time": "2024-08-28T17:04:40.145426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Simple NN\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#Preparar datos Simple NN\n",
    "def prepare_data_SimpleNN(train_data, train_labels, test_data, test_labels):\n",
    "    train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "    test_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(train_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    return train_loader, test_tensor, test_labels_tensor\n",
    "\n",
    "#Entrenar el modelo\n",
    "def train_model_SimpleNN(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs) # forward pass\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad() #backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "#Evaluar el modelo\n",
    "def evaluate_model_SimpleNN(model, test_tensor, test_labels_tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(test_tensor)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels_tensor, predicted)\n",
    "        conf_matrix = confusion_matrix(test_labels_tensor, predicted)\n",
    "        #print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "        print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.180693Z",
     "start_time": "2024-08-28T17:04:40.160806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### CNN\n",
    "class GaitCNN(nn.Module):\n",
    "    def __init__(self, input_length, num_fases):\n",
    "        super(GaitCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self._to_linear = None\n",
    "        self.convs(torch.randn(1, 1, input_length))\n",
    "        self.fc1 = nn.Linear(self._to_linear, 64)\n",
    "        self.fc2 = nn.Linear(64, num_fases)\n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.shape[1] * x.shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Función para preparar los datos\n",
    "def prepare_data_CNN(train_data, train_labels, test_data, test_labels):\n",
    "    train_data_expanded = train_data[:, None, :]  # Añadir dimensión de canales\n",
    "    test_data_expanded = test_data[:, None, :]\n",
    "    \n",
    "    #convertir a tensores\n",
    "    train_data_tensor = torch.tensor(train_data_expanded, dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "    test_data_tensor = torch.tensor(test_data_expanded, dtype=torch.float32)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "    \n",
    "    #creacion de conjuntos de datos y cargadores de datos \n",
    "    train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "    test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, test_labels_tensor, train_data_tensor\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model_CNN(cnn_model,device,criterion,optimizer, train_loader, test_loader, num_epochs=100):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        epoch_train_loss = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        train_loss.append(np.mean(epoch_train_loss))\n",
    "        \n",
    "        #validation phase\n",
    "        cnn_model.eval()\n",
    "        epoch_test_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = cnn_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_test_loss.append(loss.item())\n",
    "        \n",
    "        test_loss.append(np.mean(epoch_test_loss))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss[epoch]}, Val Loss: {test_loss[epoch]}')\n",
    "    \n",
    "    return train_loss, test_loss\n",
    "\n",
    "# Función para graficar Training and Validation loss\n",
    "def plot_loss_CNN(train_loss, test_loss, num_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_loss, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model_CNN(cnn_model, test_loader, test_labels_tensor):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    cnn_model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = cnn_model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            test_predictions.extend(predicted.cpu().detach().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(test_labels_tensor, test_predictions)\n",
    "    print(f'Accuracy of CNN: {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.199869Z",
     "start_time": "2024-08-28T17:04:40.183252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### RNN\n",
    "def preprocess_data_RNN(train_data, test_data):\n",
    "    #Asegurarse que train_data tenga 3 dimensiones\n",
    "    if len(train_data.shape) == 2:\n",
    "        train_data = train_data[:, :, np.newaxis]\n",
    "        test_data = test_data[:, :, np.newaxis]\n",
    "    return train_data, test_data\n",
    "\n",
    "class GaitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GaitRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def create_dataloaders_RNN(train_data, train_labels, test_data, test_labels, batch_size=32):\n",
    "    train_dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model_RNN(model, train_loader,test_loader, criterion, optimizer, num_epochs, device):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = []\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        train_loss.append(np.mean(epoch_train_loss))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                epoch_val_loss.append(loss.item())\n",
    "        \n",
    "        test_loss.append(np.mean(epoch_val_loss))\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss[-1]}, Val Loss: {test_loss[-1]}')\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def evaluate_model_RNN(model, test_loader,test_labels, device):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            test_predictions.extend(predicted.cpu().detach().numpy())\n",
    "    rnn_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f'Accuracy of RNN: {rnn_accuracy}')\n",
    "    #return test_predictions\n",
    "    return rnn_accuracy\n",
    "\n",
    "def plot_loss_RNN(train_loss, test_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train Loss')\n",
    "    plt.plot(range(1, len(test_loss) + 1), test_loss, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Sin Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.209129Z",
     "start_time": "2024-08-28T17:04:40.202262Z"
    }
   },
   "outputs": [],
   "source": [
    "#código de Domingo Mery\n",
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Bayesian generative classification based on KDE\n",
    "    from https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bandwidth : float\n",
    "        the kernel bandwidth within each class\n",
    "    kernel : str\n",
    "        the kernel name, passed to KernelDensity\n",
    "    \"\"\"\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        training_sets = [X[y == yi] for yi in self.classes_]\n",
    "        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                      kernel=self.kernel).fit(Xi)\n",
    "                        for Xi in training_sets]\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "                           for Xi in training_sets]\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logprobs = np.array([model.score_samples(X)\n",
    "                             for model in self.models_]).T\n",
    "        result = np.exp(logprobs + self.logpriors_)\n",
    "        return result / result.sum(1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.222382Z",
     "start_time": "2024-08-28T17:04:40.211577Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_classifiers():\n",
    "    return {\n",
    "    'knn                               ': KNeighborsClassifier(n_neighbors=1),\n",
    "    'knn-3                             ': KNeighborsClassifier(n_neighbors=3),\n",
    "    'knn-5                             ': KNeighborsClassifier(n_neighbors=5),\n",
    "    'knn-8                             ': KNeighborsClassifier(n_neighbors=8),\n",
    "    'knn-9                             ': KNeighborsClassifier(n_neighbors=9),\n",
    "    'knn-10                            ': KNeighborsClassifier(n_neighbors=10),\n",
    "    'knn-15                            ': KNeighborsClassifier(n_neighbors=15),\n",
    "    'knn-20                            ': KNeighborsClassifier(n_neighbors=15),\n",
    "    'mlp                               ': MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
    "    'mlp layers 2                      ': MLPClassifier(hidden_layer_sizes=(100, 50), alpha=1, max_iter=1000, random_state=42),\n",
    "    'svm lineal 1                      ': SVC(kernel='linear', gamma=0.2, C=0.1),\n",
    "    'svm lineal 2                      ': SVC(kernel='linear', gamma=0.25, C=0.2),\n",
    "    'svm polinomial                    ': SVC(kernel='poly', gamma=0.2,degree=3, C=0.1),\n",
    "    'svm rbf 1                         ': SVC(kernel='rbf', gamma=0.2, C=0.1),\n",
    "    'svm rbf 2                         ': SVC(kernel='rbf', gamma=0.65, C=0.25),\n",
    "    'svm rbf 3                         ': SVC(kernel=\"rbf\", random_state=42),\n",
    "    'svm rbf gamma auto                ': SVC(kernel='rbf', gamma='auto'),\n",
    "    'svm sigmoidal                     ': SVC(kernel='sigmoid', gamma=0.01, C=1.5),\n",
    "    'dmin                              ': NearestCentroid(),\n",
    "    'bayes kde                         ': KDEClassifier(bandwidth=1.0),\n",
    "    'naive bayes                       ': GaussianNB(),\n",
    "    'lda                               ': LinearDiscriminantAnalysis(),\n",
    "    'qda                               ': QuadraticDiscriminantAnalysis(),\n",
    "    'random forest depth 3             ': RandomForestClassifier(max_depth=3,n_estimators=150),\n",
    "    'random forest depth 15            ': RandomForestClassifier(max_depth=15,n_estimators=150),\n",
    "    'random forest depth 30            ': RandomForestClassifier(max_depth=30, n_estimators=200),\n",
    "    'random forest depth 100           ': RandomForestClassifier(max_depth=100,n_estimators=150),\n",
    "    'random forest n_estimators 300    ': RandomForestClassifier(n_estimators=300),\n",
    "    'decision tree                     ': DecisionTreeClassifier(max_depth=3),\n",
    "    'decision tree depth 12            ': DecisionTreeClassifier(max_depth=12),\n",
    "    'decision tree depth 100           ': DecisionTreeClassifier(max_depth=100),\n",
    "    'logistic regression lbfgs         ': LogisticRegression(C=0.1,solver=\"lbfgs\"),\n",
    "    'logistic regression newton-cg     ': LogisticRegression(C=0.2,solver=\"newton-cg\"),\n",
    "    'gradient boosting                 ': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42),\n",
    "    'adaboost                          ': AdaBoostClassifier(n_estimators=100),\n",
    "    'extra trees                       ': ExtraTreesClassifier(n_estimators=100),\n",
    "    'bagging                           ': BaggingClassifier(n_estimators=50),\n",
    "    'perceptron                        ': Perceptron(max_iter=1000),\n",
    "    'sgd                               ': SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.228523Z",
     "start_time": "2024-08-28T17:04:40.225542Z"
    }
   },
   "outputs": [],
   "source": [
    "def clasificacion(model, train_data, train_labels, test_data):\n",
    "    model.fit(train_data, train_labels) #Se clasifica el Testing\n",
    "    y_pred = model.predict(test_data)  #Se calcula el Accuracy en el Testing y se almacena en acc[i]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.234920Z",
     "start_time": "2024-08-28T17:04:40.231375Z"
    }
   },
   "outputs": [],
   "source": [
    "def crossval(clf,X,y,nfolds=10,show=1):\n",
    "  scores = cross_val_score(clf, X, y, cv=nfolds)\n",
    "  acc = np.mean(scores)\n",
    "  if show:\n",
    "    acc_st = \"{:.2f}\".format(acc*100)\n",
    "    print('Accuracy = '+str(acc_st))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.242229Z",
     "start_time": "2024-08-28T17:04:40.237377Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_classifiers(classifiers, features, labels, nfolds=10, use_sfs=False,n_features=20):\n",
    "    if use_sfs:\n",
    "        sfs_indices = sfs(features, labels, n_features)\n",
    "        features = features[:, sfs_indices]\n",
    "        print(f'Selected Features: {sfs_indices}')\n",
    "\n",
    "    accuracies = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nEvaluando: {name.strip()}\")\n",
    "        accuracies[name] = crossval(clf, features, labels, nfolds=nfolds, show=1)\n",
    "    \n",
    "    print(\"\\nPrecisión total por clasificador:\")\n",
    "    for name, acc in accuracies.items():\n",
    "        print(f'{name.strip()}: {acc:.2f}')\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.247721Z",
     "start_time": "2024-08-28T17:04:40.244538Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_accuracies(accuracies):\n",
    "    print(\"\\nAccuracy Total por Clasificador:\")\n",
    "    for name, acc in accuracies.items():\n",
    "        print(f\"{name}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solo clean y normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.253409Z",
     "start_time": "2024-08-28T17:04:40.249813Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_cleannormalize(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = clean_normalized_feature_selection(train_data, test_data)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.267114Z",
     "start_time": "2024-08-28T17:04:40.256066Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_sfs5(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = sfs_selection(train_data, test_data, train_labels,5)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    return accuracies\n",
    "\n",
    "def evaluate_model_sfs10(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = sfs_selection(train_data, test_data, train_labels,10)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_sfs15(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = sfs_selection(train_data, test_data, train_labels,15)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_sfs20(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = sfs_selection(train_data, test_data, train_labels,20)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_sfs25(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = sfs_selection(train_data, test_data, train_labels,25)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.280264Z",
     "start_time": "2024-08-28T17:04:40.270374Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_pca10(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = pca_tranformer(train_data, test_data,10)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_pca20(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = pca_tranformer(train_data, test_data,20)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_pca25(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = pca_tranformer(train_data, test_data,25)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.291295Z",
     "start_time": "2024-08-28T17:04:40.283786Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_ica10(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = ica_tranformer(train_data, test_data, train_labels,10)\n",
    "\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_ica20(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = ica_tranformer(train_data, test_data, train_labels,20)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_ica25(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = ica_tranformer(train_data, test_data, train_labels,25)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T17:04:40.301182Z",
     "start_time": "2024-08-28T17:04:40.293575Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_plsr10(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = plsr_transformer(train_data, test_data, train_labels,10)\n",
    "\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_plsr20(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = plsr_transformer(train_data, test_data, train_labels,20)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "def evaluate_model_plsr25(train_data, test_data, train_labels, test_labels, classifiers):\n",
    "    accuracies = {name: [] for name in classifiers.keys()}\n",
    "    train_data, test_data = plsr_transformer(train_data, test_data, train_labels,25)\n",
    "   \n",
    "    for name, clf in classifiers.items():\n",
    "        ypred = clasificacion(clf, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, ypred)\n",
    "        accuracies[name].append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
